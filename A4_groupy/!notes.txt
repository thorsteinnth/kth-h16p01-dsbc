Group membership service
Atomic multicast

Nodes with leader
Send message to leader - leader does basic multicast to all - if leader dies elect new one

Node join - send request to any node - leader decides when to include it and send a new view of system to all nodes

Application layer has a group process that communication goes through - no views go to the application layer

View synchrony
"messages are delivered in a view"
For all messages in a view we guarantee:
- FIFO - in the order they were sent by the sending node
- Total order - all nodes see the same sequence
- Reliable - If a correct node delivers a message all correct nodes deliver the message

Correct node:
Does not fail during a view, i.e. it survives to install the next view
(a node will fail only by crashing and will then never be heard from again)

NOTE:
Not guaranteed that a send message is delivered. Async sending and no ACKs.
If failing leader then a sent message might disappear.

Leader:
Node is either leader or slave
One leader (hopefully)
All slaves forward messages to leader, leader tags it with seqnum, multicast it to all other nodes
Leader can also accept messages from its own master - the application layer
NOTE: Application layer does not know if its group is a leader or a slave

Slave:
Receive messages from application layer and forward them to the leader.
Receive messages from leader and forward them to the application layer.
Have to be able to deal with it if the leader fails.

Election:
All slaves have same list of peers.
First node in the list is the leader.
If I am the leader I resend the last message I received.
Slaves monitor the new leader.

Application layer:
Create a group process
Contact any other application layer process it knows of
- Request to join the group, send the PID of its own group process
- Wait for view delivery, containing the peer processes in the group
No guarantee that the join request makes it - the leader might be dead, or the request might not be delivered to the
leader
- application layer process just timeouts and aborts the attempt
After joining, must get current state of group (color)
Sends request to obtain state to atomic multicast layer, and waits to receive the message from itself
- now I know that the other processes see this message and respond by sending the state, also using the multicast layer
NOTE:
state message might not be the first message we receive
state change messages might be in the pipeline
after receiving the state message these state change messages must be applied before the process is up and running
simply let any state change messages remain in the queue an choose to handle state message first, before
state change messages (using erlang's implicit deferral)

-----------------------------------------------------------------------------------------------------------------------

TODO:
One thing that we have to pay attention to is what we should do if, as a slave,
receive the view message from the new leader before we have noticed that the old leader is dead.
Should we refuse to handle view messages unless we have seen the Down message from the leader or should we happily
receive accept the new view and then ignore trailing Down messages.
- Right now we just monitor a new leader when we get the Down message

gms1 testing:
After killing leader they just stop changing colors. They try to multicast a color change, do that by sending a message
to the leader, but the leader is dead. So that message never goes anywhere.

gms2 testing:
After killing the leader, the next node becomes the leader. The remaining nodes keep changing colors, and stay
in sync.

-----------------------------------------------------------------------------------------------------------------------

missing messages testing:
introduced random crash. Am supposed to get the state of the workers out of sync.
What is happening?
Am getting them out of sync. Got that by having 4 workers.

NOPE.
Something to do with the fact that they are just starting to monitor a new leader when they get the Down message.
Don't actually start monitoring a new leader when they get the view message?
NOPE.

Þeir eru sammála um hver er leaderinn, samt er þetta að gerast.

7> test:test_gms2_random_crash().
Will kill all processes in 90 sec
leader 1: crash
[4][<0.141.0>] LEADER IS: <0.139.0>
[5][<0.142.0>] LEADER IS: <0.139.0>
[3][<0.140.0>] LEADER IS: <0.139.0>
[2][<0.139.0>] LEADER IS: myself
leader 2: crash
[4][<0.141.0>] LEADER IS: <0.140.0>
[3][<0.140.0>] LEADER IS: myself
[5][<0.142.0>] LEADER IS: <0.140.0>
leader 3: crash
[5][<0.142.0>] LEADER IS: <0.141.0>
[4][<0.141.0>] LEADER IS: myself
leader 4: crash
[5][<0.142.0>] LEADER IS: myself
stop

-------------------------------

Ef leaderinn er að senda út á alla slaves, en deyr svo áður en hann nær að senda á alla (MISSING MESSAGES),
þá fá ekki allir skilaboðin um litabreytinguna. ATH að litabreytingin byggir alltaf á núverandi lit ((R+N) rem 256),
þannig að þeir syncast ekki saman þótt þeir verði svo sammála um nýja leaderinn.

Node1: 100
Node2: 100

Node1: 100
Node2: 100+10

Node1: 100+50 = 150
Node2: 110+50 = 160

Node1: 150+30 = 180
Node2: 160+30 = 190

Sama þótt þeir séu núna í synci með öll skilaboð sem þeir fá úr þessu, þá er Node2 alltaf með 10 hærra R gildi
-> Alltaf mismunandi litir.

--------------------------------

[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.197.0>, MESSAGE: {msg,
                                                                 {change,6}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,6}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.197.0>, MESSAGE: {msg,
                                                                 {change,7}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,7}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.197.0>, MESSAGE: {msg,
                                                                 {change,20}}

!!!CRASH BEFORE IT CAN SEND THE MESSAGE TO 198!!!

leader 3: crash
[5][<0.198.0>] LEADER IS: <0.197.0>
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {view,
                                                                 [<0.197.0>,
                                                                  <0.198.0>],
                                                                 [<0.191.0>,
                                                                  <0.192.0>]}
[4][<0.197.0>] LEADER IS: myself
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,14}}
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,10}}
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,10}}

Node 198 is now missing the {change,20} message, so it will be out of sync with the color progression of node 197
(the leader).

-----------------------------------------------------------------------------------------------------------------------

Reliable multicast to fix the missing messages problem.

Vanilla reliable multicaster (process that forwards all messages before delivering them to a higher layer) would be
expensive.

Keep copy of last message from leader.
If we detect the death of the leader then some nodes may never have gotten this message (if he was in the middle
of the basic multicast procedure).
Assume:
- Messages are reliably delivered
-> if leader sends message to A and then B, if B receives the message, then also A will receive it (the leader can't have
died before sending to A, it must have been able to send there since it was able to send to B, note that the only way we
lose messages is if the leader dies, because of the reliable delivery condition)

Leader is sending messages out in the order that the slaves are in in the list of peers.
So, if anyone receives a message before the leader crashes while multicasting, it is guaranteed that the next
leader has received the message (he is the first node in the list of peers).
(either him and no one else, or him and an arbitrary number of other nodes).
-> only the next leader needs to resend the message

This way a node can receive duplicate messages
New leader + node X received message before the leader dying
New leader resends message to all nodes
Node X receives a duplicate
-> Number all messages and only deliver new messages to the application layer

Note that if the leader dies before it has the chance to send out any messages, we do not have a problem.
A new leader will be elected and the nodes will not go out of sync, since none of them received the change color
(increment color actually) message.

Not getting them in sync

[4][<0.78.0>] RECEIVED MESSAGE: {msg,53,{change,1}}
[5][<0.79.0>] RECEIVED MESSAGE: {msg,53,{change,1}}
[6][<0.80.0>] RECEIVED MESSAGE: {msg,53,{change,1}}
leader 3: crash
[4][<0.78.0>] RECEIVED MESSAGE: {msg,54,{change,5}}
[5][<0.79.0>] RECEIVED MESSAGE: {msg,54,{change,5}}
[6][<0.80.0>] RECEIVED MESSAGE: {msg,54,{change,5}}

!!!EVERYBODY AT 54

[4][<0.78.0>] LEADER IS: myself
[5][<0.79.0>] LEADER IS: <0.78.0>
[6][<0.80.0>] LEADER IS: <0.78.0>

!!!RESENDING 54

[4][<0.78.0>] NEW LEADER BROADCASTING LAST RECEIVED MSG: {msg,54,{change,5}}
[5][<0.79.0>] RECEIVED MESSAGE: {msg,54,{change,5}}
[6][<0.80.0>] RECEIVED MESSAGE: {msg,54,{change,5}}

!!! THE OTHER NODES WILL PERFORM THIS COLOR CHANGE! THE CRITERIA IS IF I<N
!!! NOW THE TWO SLAVES ARE AHEAD OF THE LEADER IN THE COLOR PROGRESSION

[5][<0.79.0>] RECEIVED MESSAGE: {view,55,
                                      [<0.78.0>,<0.79.0>,<0.80.0>],
                                      [<0.72.0>,<0.73.0>,<0.74.0>]}
[6][<0.80.0>] RECEIVED MESSAGE: {view,55,
                                      [<0.78.0>,<0.79.0>,<0.80.0>],
                                      [<0.72.0>,<0.73.0>,<0.74.0>]}
[5][<0.79.0>] RECEIVED MESSAGE: {msg,56,{change,2}}
[6][<0.80.0>] RECEIVED MESSAGE: {msg,56,{change,2}}
[5][<0.79.0>] RECEIVED MESSAGE: {msg,57,{change,11}}
[6][<0.80.0>] RECEIVED MESSAGE: {msg,57,{change,11}}

NOTE
ERROR IN ASSIGNMENT DOC
we discard messages when I <= N, where I is the incoming seqnum, and N is our seqnum (i.e. the seqnum of the last
message we accepted).
The assignment doc has I < N.

4][<0.122.0>] RECEIVED MESSAGE: {msg,52,{change,15}}
[5][<0.123.0>] RECEIVED MESSAGE: {msg,52,{change,15}}
[6][<0.124.0>] RECEIVED MESSAGE: {msg,52,{change,15}}
[4][<0.122.0>] RECEIVED MESSAGE: {msg,53,{change,5}}
[5][<0.123.0>] RECEIVED MESSAGE: {msg,53,{change,5}}
[6][<0.124.0>] RECEIVED MESSAGE: {msg,53,{change,5}}
leader 3: crash
[4][<0.122.0>] RECEIVED MESSAGE: {msg,54,{change,17}}
[5][<0.123.0>] RECEIVED MESSAGE: {msg,54,{change,17}}
[6][<0.124.0>] RECEIVED MESSAGE: {msg,54,{change,17}}
[4][<0.122.0>] LEADER IS: myself
[5][<0.123.0>] LEADER IS: <0.122.0>
[6][<0.124.0>] LEADER IS: <0.122.0>
[4][<0.122.0>] NEW LEADER BROADCASTING LAST RECEIVED MSG: {msg,54,{change,17}}
[5][<0.123.0>] DISCARDING MESSAGE WITH SEQNUM: 54, CURRENT SEQNUM: 54, MESSAGE: {change,
                                                                                 17}
[6][<0.124.0>] DISCARDING MESSAGE WITH SEQNUM: 54, CURRENT SEQNUM: 54, MESSAGE: {change,
                                                                                 17}
[5][<0.123.0>] RECEIVED MESSAGE: {view,55,
                                     [<0.122.0>,<0.123.0>,<0.124.0>],
                                     [<0.115.0>,<0.116.0>,<0.117.0>]}
[6][<0.124.0>] RECEIVED MESSAGE: {view,55,
                                     [<0.122.0>,<0.123.0>,<0.124.0>],
                                     [<0.115.0>,<0.116.0>,<0.117.0>]}
[5][<0.123.0>] RECEIVED MESSAGE: {msg,56,{change,7}}
[6][<0.124.0>] RECEIVED MESSAGE: {msg,56,{change,7}}
[5][<0.123.0>] RECEIVED MESSAGE: {msg,57,{change,1}}
[6][<0.124.0>] RECEIVED MESSAGE: {msg,57,{change,1}}
[5][<0.123.0>] RECEIVED MESSAGE: {msg,58,{change,2}}
[6][<0.124.0>] RECEIVED MESSAGE: {msg,58,{change,2}}

OK NOW.














