Group membership service
Atomic multicast

Nodes with leader
Send message to leader - leader does basic multicast to all - if leader dies elect new one

Node join - send request to any node - leader decides when to include it and send a new view of system to all nodes

Application layer has a group process that communication goes through - no views go to the application layer

View synchrony
"messages are delivered in a view"
For all messages in a view we guarantee:
- FIFO - in the order they were sent by the sending node
- Total order - all nodes see the same sequence
- Reliable - If a correct node delivers a message all correct nodes deliver the message

Correct node:
Does not fail during a view, i.e. it survives to install the next view
(a node will fail only by crashing and will then never be heard from again)

NOTE:
Not guaranteed that a send message is delivered. Async sending and no ACKs.
If failing leader then a sent message might disappear.

Leader:
Node is either leader or slave
One leader (hopefully)
All slaves forward messages to leader, leader tags it with seqnum, multicast it to all other nodes
Leader can also accept messages from its own master - the application layer
NOTE: Application layer does not know if its group is a leader or a slave

Slave:
Receive messages from application layer and forward them to the leader.
Receive messages from leader and forward them to the application layer.
Have to be able to deal with it if the leader fails.

Election:
All slaves have same list of peers.
First node in the list is the leader.
If I am the leader I resend the last message I received.
Slaves monitor the new leader.

Application layer:
Create a group process
Contact any other application layer process it knows of
- Request to join the group, send the PID of its own group process
- Wait for view delivery, containing the peer processes in the group
No guarantee that the join request makes it - the leader might be dead, or the request might not be delivered to the
leader
- application layer process just timeouts and aborts the attempt
After joining, must get current state of group (color)
Sends request to obtain state to atomic multicast layer, and waits to receive the message from itself
- now I know that the other processes see this message and respond by sending the state, also using the multicast layer
NOTE:
state message might not be the first message we receive
state change messages might be in the pipeline
after receiving the state message these state change messages must be applied before the process is up and running
simply let any state change messages remain in the queue an choose to handle state message first, before
state change messages (using erlang's implicit deferral)

-----------------------------------------------------------------------------------------------------------------------

TODO:
One thing that we have to pay attention to is what we should do if, as a slave,
receive the view message from the new leader before we have noticed that the old leader is dead.
Should we refuse to handle view messages unless we have seen the Down message from the leader or should we happily
receive accept the new view and then ignore trailing Down messages.
- Right now we just monitor a new leader when we get the Down message

gms1 testing:
After killing leader they just stop changing colors. They try to multicast a color change, do that by sending a message
to the leader, but the leader is dead. So that message never goes anywhere.

gms2 testing:
After killing the leader, the next node becomes the leader. The remaining nodes keep changing colors, and stay
in sync.

-----------------------------------------------------------------------------------------------------------------------

missing messages testing:
introduced random crash. Am supposed to get the state of the workers out of sync.
What is happening?
Am getting them out of sync. Got that by having 4 workers.

NOPE.
Something to do with the fact that they are just starting to monitor a new leader when they get the Down message.
Don't actually start monitoring a new leader when they get the view message?
NOPE.

Þeir eru sammála um hver er leaderinn, samt er þetta að gerast.

7> test:test_gms2_random_crash().
Will kill all processes in 90 sec
leader 1: crash
[4][<0.141.0>] LEADER IS: <0.139.0>
[5][<0.142.0>] LEADER IS: <0.139.0>
[3][<0.140.0>] LEADER IS: <0.139.0>
[2][<0.139.0>] LEADER IS: myself
leader 2: crash
[4][<0.141.0>] LEADER IS: <0.140.0>
[3][<0.140.0>] LEADER IS: myself
[5][<0.142.0>] LEADER IS: <0.140.0>
leader 3: crash
[5][<0.142.0>] LEADER IS: <0.141.0>
[4][<0.141.0>] LEADER IS: myself
leader 4: crash
[5][<0.142.0>] LEADER IS: myself
stop

-------------------------------

Ef leaderinn er að senda út á alla slaves, en deyr svo áður en hann nær að senda á alla (MISSING MESSAGES),
þá fá ekki allir skilaboðin um litabreytinguna. ATH að litabreytingin byggir alltaf á núverandi lit ((R+N) rem 256),
þannig að þeir syncast ekki saman þótt þeir verði svo sammála um nýja leaderinn.

Node1: 100
Node2: 100

Node1: 100
Node2: 100+10

Node1: 100+50 = 150
Node2: 110+50 = 160

Node1: 150+30 = 180
Node2: 160+30 = 190

Sama þótt þeir séu núna í synci með öll skilaboð sem þeir fá úr þessu, þá er Node2 alltaf með 10 hærra R gildi
-> Alltaf mismunandi litir.

--------------------------------

[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.197.0>, MESSAGE: {msg,
                                                                 {change,6}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,6}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.197.0>, MESSAGE: {msg,
                                                                 {change,7}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,7}}
[3][<0.196.0>] LEADER SENT MESSAGE TO NODE: <0.197.0>, MESSAGE: {msg,
                                                                 {change,20}}

!!!CRASH BEFORE IT CAN SEND THE MESSAGE TO 198!!!

leader 3: crash
[5][<0.198.0>] LEADER IS: <0.197.0>
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {view,
                                                                 [<0.197.0>,
                                                                  <0.198.0>],
                                                                 [<0.191.0>,
                                                                  <0.192.0>]}
[4][<0.197.0>] LEADER IS: myself
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,14}}
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,10}}
[4][<0.197.0>] LEADER SENT MESSAGE TO NODE: <0.198.0>, MESSAGE: {msg,
                                                                 {change,10}}

Node 198 is now missing the {change,20} message, so it will be out of sync with the color progression of node 197
(the leader).

-----------------------------------------------------------------------------------------------------------------------